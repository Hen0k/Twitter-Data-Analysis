{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719c95ae-43e6-4861-9015-18600789bbef",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e86d1-25f4-41bc-9f00-b2be8a365810",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4515c2-6e40-444b-b05a-92ea202d3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (precision_score, \n",
    "                             recall_score, \n",
    "                             accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c913d2d-37b1-4845-ae1f-f6c1a176175c",
   "metadata": {},
   "source": [
    "We need some code that is inside the parent directory of where we are. Inorder for import to work I will add the\n",
    "parent directory to the system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba1b9a5-ca61-4fc1-9501-80d42048741a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hat/dev-env/10Acadamy/week_0/Twitter-Data-Analysis/notebooks'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9b67c05-4dbb-4475-a7b5-3ec221e39fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "parent = cwd.replace('/notebooks', '')\n",
    "sys.path.insert(0, parent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0925bb2-7916-49f9-8dc0-34e455b1ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_dataframe import read_json\n",
    "from extract_dataframe import TweetDfExtractor\n",
    "from clean_tweets_dataframe import CleanTweets\n",
    "from tweets_preprocess import SADataPreparation\n",
    "from utils import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bb656-369f-480c-baf5-22970b5d5986",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507a793-0e1a-4f84-8b82-b33fc8f3576f",
   "metadata": {},
   "source": [
    "Here, what we have are tweets. I will be considering each tweet as a document.\n",
    "Let's follow the following steps:\n",
    "- Read the cached CSV\n",
    "- Use the `SADataPreparation` class to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "384cade8-e734-4fe0-a6c3-597bcc7728ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'processed_tweet_data.csv'\n",
    "loader = DataLoader('../', filename)\n",
    "df = loader.read_csv()\n",
    "# print(df.shape)\n",
    "# print(df.columns)\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83efaa0a-5c4d-487b-a2a0-154da2d8a85a",
   "metadata": {},
   "source": [
    "## Data Cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30a499-efb8-4f98-a880-9e78e0f6c864",
   "metadata": {},
   "source": [
    "We have imported the `CleanTweets` class for this task. We will use the `run_pipeline` method to automate the cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b46cb63-58ea-4e92-861d-9c2a43fdc077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automation in Action...!!!\n"
     ]
    }
   ],
   "source": [
    "cleaner = CleanTweets()\n",
    "cleaned_df = cleaner.run_pipeline(df, save_csv=True)\n",
    "# cleaned_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c6e61-862f-40f5-9b33-6905fb76b008",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245f04c-ad2c-4c3a-b573-826ca7e18ba5",
   "metadata": {},
   "source": [
    "The data needs to be in an X ---> Y format.\n",
    "The X would be the clean_text and Y would be a label calculated \n",
    "from polarity and subjectivity\n",
    "\n",
    "This is exactly what the `SADataPreparation` class will do for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1be06e7-f704-44a6-bb2e-14795a9d79c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing the Tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8531, 8531, 2133, 2133)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the train test split of the labeled data \n",
    "X_train, X_test, y_train, y_test = SADataPreparation().prepare_features(cleaned_df, drop_neutral=False)\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53362652-d921-46a5-8a05-cb13ed24eadb",
   "metadata": {},
   "source": [
    "After getting the labled data we will vectorize it in a way a model would understand. Here I will use a Count vector and a Term-Frequnecy Inverse-Document-Frequency of our data and see which is better for the task. I will also be using both a unigram and bigram vectorization of the labled data. \n",
    "\n",
    "In total I will have four data formats\n",
    "\n",
    "But first we need a base mode to test with, I will use an SGDClassifer.\n",
    "I am also using the methods specified in this blog: https://machinelearningmastery.com/grid-search-data-preparation-techniques/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "161799c5-d8f2-4bf5-9617-8dcd13414d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipelines(base_model):\n",
    "    pipelines = list()\n",
    "    # Unigram count vector\n",
    "    p = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,1))), \n",
    "        ('model', base_model)\n",
    "    ])\n",
    "    pipelines.append(('unigram_count_vectorizer', p))\n",
    "    # Bigram count vector\n",
    "    p = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,2))), \n",
    "        ('model', base_model)\n",
    "    ])\n",
    "    pipelines.append(('bigram_count_vectorizer', p))\n",
    "    # Term Frequency unigram\n",
    "    p = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,1))),\n",
    "        ('tf_vectorize', TfidfTransformer(use_idf=False)), \n",
    "        ('model', base_model)\n",
    "    ])\n",
    "    pipelines.append(('unigram_tf_vectorizer', p))\n",
    "    # Term Frequency bigram\n",
    "    p = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,2))),\n",
    "        ('tf_vectorize', TfidfTransformer(use_idf=False)), \n",
    "        ('model', base_model)\n",
    "    ])\n",
    "    pipelines.append(('bigram_tf_vectorizer', p))\n",
    "    #  Term Frequency Inverse Document Frequecy unigram\n",
    "    p = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,1))),\n",
    "        ('tf_vectorize', TfidfTransformer(use_idf=True)), \n",
    "        ('model', base_model)\n",
    "    ])\n",
    "    pipelines.append(('unigram_tfidf_vectorizer', p))\n",
    "    #  Term Frequency Inverse Document Frequecy bigram\n",
    "    p = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,2))),\n",
    "        ('tf_vectorize', TfidfTransformer(use_idf=True)), \n",
    "        ('model', base_model)\n",
    "    ])\n",
    "    pipelines.append(('bigram_tfidf_vectorizer', p))\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b809a35d-b28a-4598-a76e-bcfe449a23cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### unigram_count_vectorizer Test mean accuracy:\t0.8096577590248476\n",
      "### bigram_count_vectorizer Test mean accuracy:\t0.8166901078293484\n",
      "### unigram_tf_vectorizer Test mean accuracy:\t0.8054383497421472\n",
      "### bigram_tf_vectorizer Test mean accuracy:\t0.8049695264885138\n",
      "### unigram_tfidf_vectorizer Test mean accuracy:\t0.802625410220347\n",
      "### bigram_tfidf_vectorizer Test mean accuracy:\t0.7984060009376465\n",
      "\n",
      ">>> The best pipeline based on the above results is\n",
      "('bigram_count_vectorizer', Pipeline(steps=[('count_vectorize', CountVectorizer(ngram_range=(1, 2))),\n",
      "                ('model', SGDClassifier())]))\n"
     ]
    }
   ],
   "source": [
    "# base model\n",
    "# base_model = MultinomialNB()\n",
    "base_model = SGDClassifier()\n",
    "# base_model = Perceptron()\n",
    "# base_model = DecisionTreeClassifier()\n",
    "\n",
    "# get the modeling pipelines\n",
    "pipelines = get_pipelines(base_model)\n",
    "# evaluate each pipeline\n",
    "best_pipeline = None\n",
    "max_acc = 0\n",
    "for name, pipeline in pipelines:\n",
    "    # train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # predict\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    # measure\n",
    "    test_accuracy = mean(predictions==y_test)\n",
    "    if test_accuracy > max_acc:\n",
    "        max_acc = test_accuracy\n",
    "        best_pipeline = (name, pipeline)\n",
    "    print(f\"### {name} Test mean accuracy:\\t{test_accuracy}\")\n",
    "\n",
    "print(f\"\\n>>> The best pipeline based on the above results is\\n{best_pipeline}\")\n",
    "\n",
    "best_pipeline = best_pipeline[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae7d9a-8cd4-43b9-910e-1f34bde43f6a",
   "metadata": {},
   "source": [
    "The bes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dc112bf-cffb-431a-ab40-401fee3fe106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0b43caf-03d8-4e0d-9a12-e75a1fafa71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd11d8e-73c6-47f3-b5a2-73bec440824d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85daa047-d109-4efc-a98a-7cd4a81d847e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63cf752d-ef56-4a28-a1b1-e34b3bc47c3a",
   "metadata": {},
   "source": [
    "# Parameter tuning using grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f009cad-22b6-41c0-b4a2-b18ab1923447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'model__loss': 'log', 'model__penalty': 'l1'}\n",
      "Best score: 0.8166697915142874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "sentiment_clf = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,1))), \n",
    "        ('model', SGDClassifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    # 'model__alpha': (1e-2, 1e-3),\n",
    "    'model__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'model__loss': ['hinge', 'log', 'perceptron'],\n",
    "    # 'model__learning_rate': ['optimal', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_cv = GridSearchCV(\n",
    "    sentiment_clf,\n",
    "    parameters,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_cv.fit(X_train[:], y_train[:])\n",
    "print(f'Best params: {grid_search_cv.best_params_}')\n",
    "print(f'Best score: {grid_search_cv.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a8ad0da-64ba-43a4-8f00-c07a05a1bf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model__loss: 'log'\n",
      "model__penalty: 'l1'\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid_search_cv.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "617a2769-5fda-4fb5-88cb-e827a1082571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processing the Tweets\n"
     ]
    }
   ],
   "source": [
    "# Get Vectorized Features\n",
    "X_train_count, X_test_count, y_train, y_test, count_vect = SADataPreparation().vectorize_features(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8c7ba-2ffe-4fce-8fa1-fe468e12155d",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0db7ba69-0d9e-4e27-a825-e0048c5d03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    def __init__(self,X_train,X_test,y_train,y_test, vectorizor):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.vectorizor = vectorizor\n",
    "        self.trained_model = None\n",
    "\n",
    "    def model(self):\n",
    "        self.trained_model = SGDClassifier(loss=grid_search_cv.best_params_['model__loss'], \n",
    "                            learning_rate=\"optimal\", \n",
    "                            penalty=grid_search_cv.best_params_['model__penalty'])\n",
    "        \n",
    "        # self.vectorizor.fit(self.X_train)\n",
    "        # self.X_train = self.vectorizor.transform(self.X_train)\n",
    "        \n",
    "        self.trained_model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # return clf\n",
    "\n",
    "    def predict(self, x_test=None):\n",
    "        self.model()\n",
    "        if x_test is None:\n",
    "            \n",
    "            return self.trained_model.predict(self.X_test)\n",
    "        else:\n",
    "            x_vect = self.vectorizor.transform(x_test)\n",
    "            \n",
    "            return self.trained_model.predict(x_vect)\n",
    "\n",
    "    def display_model_info(self):\n",
    "        print(self.trained_model.show_topics(formatted=False))\n",
    "\n",
    "    def score(self):\n",
    "        train_score = self.trained_model.score(self.predict(self.X_train), self.y_train)\n",
    "        test_score = self.trained_model.score(self.predict(self.X_test), self.y_test)\n",
    "        print(\"Train score: \"+str(round(train_score, 2))+\" ; Validation score: \"+str(round(test_score, 2)))\n",
    "\n",
    "    def precision_recall(self):\n",
    "        precision = precision_score(self.y_test, self.predict(), average='weighted')\n",
    "        recall = recall_score(self.y_test, self.predict(), average='weighted')\n",
    "        print(f'The precision score is {precision} and the recall score is {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741868a-7e30-4537-b1f6-a160dff9fe5d",
   "metadata": {},
   "source": [
    "There are lots of missing values in the sensitivity colum, so i will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b77430c-0249-4a89-aa85-54dca14b6829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1,  1,  0,  1,  1,  1,  0,  1,  1,  1,  0, -1,  0, -1,  1,\n",
       "        1,  1,  1,  0, -1,  0,  1, -1,  0,  0,  1,  0,  1,  0,  0,  1,  0,\n",
       "        1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  1,\n",
       "        0,  1,  0,  1,  1,  0, -1,  0,  0,  0,  0,  1,  0,  1,  1, -1, -1,\n",
       "        1, -1,  0,  0,  1,  1,  0,  1, -1,  0,  1,  1,  1,  0,  1,  0,  1,\n",
       "        0,  0,  1,  1, -1,  0,  1,  0,  0,  0,  1,  0,  0,  1,  0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_pipe = Pipeline([\n",
    "        ('count_vectorize', CountVectorizer(ngram_range=(1,1)))\n",
    "])\n",
    "model_output = Modeling(X_train_count, X_test_count, y_train, y_test, count_vect)\n",
    "model_output.model()\n",
    "model_output.predict()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de56257e-6b4f-4ae8-825a-5d6ff376c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.predict([\"I love this lunch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8b1d294-c271-41f3-add3-5494db37f480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score is 0.8287983320099397 and the recall score is 0.829817158931083\n"
     ]
    }
   ],
   "source": [
    "model_output.precision_recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9a469-f8e9-4ac3-866e-5263d7e1dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labled_df = SADataPreparation().preprocess_data(cleaned_df, False)\n",
    "labled_df = labled_df[['clean_text', 'score']]\n",
    "labled_df['score'].value_counts().plot.pie(figsize=(7,7), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102f515-a9b0-4d50-b8c3-1bd0c1f21610",
   "metadata": {},
   "source": [
    "Next up: Read this blog and use some of the techniques used to find the best parametrs and get the best results.\n",
    "blog link: https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
